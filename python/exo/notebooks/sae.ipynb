{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6deef11f-7bf4-4755-a561-7adb67a5c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install --upgrade sae-lens transformer-lens sae-dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80461223-b29d-46d5-8be9-071d3cd01ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Int, Float\n",
    "\n",
    "import torch, pathlib, pandas as pd\n",
    "import huggingface_hub as hf_hub, safetensors as st\n",
    "\n",
    "# device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e6fa02-881f-4c9b-b10d-44f1d76057cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c9773-e4c6-418c-b383-5c1170ba165e",
   "metadata": {},
   "source": [
    "---\n",
    "## metadata for SAE exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7b32d3-93a4-4428-baf9-5ea58c91c119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records(\n",
    "    {k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}\n",
    ").T\n",
    "df.drop(\n",
    "    columns=[\"expected_var_explained\",\"expected_l0\",\n",
    "             \"config_overrides\",\"conversion_func\"],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# currently only layer 6 works pretty well\n",
    "layer = 20\n",
    "MODEL = \"google/gemma-2-9b-it\"\n",
    "SAE_ID=\"gemma-scope-9b-it-res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a095db29-a5cc-4afc-a9da-11535a96861f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>model</th>\n",
       "      <th>saes_map</th>\n",
       "      <th>neuronpedia_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-2b-it-res-jb</th>\n",
       "      <td>gemma-2b-it-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-IT-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-it</td>\n",
       "      <td>{'blocks.12.hook_resid_post': 'gemma_2b_it_blo...</td>\n",
       "      <td>{'blocks.12.hook_resid_post': 'gemma-2b-it/12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-2b-res-jb</th>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma_2b_blocks....</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res</th>\n",
       "      <td>gemma-scope-27b-pt-res</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': 'layer_...</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': None, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res-canonical</th>\n",
       "      <td>gemma-scope-27b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "      <td>{'layer_10/width_131k/canonical': 'layer_10/wi...</td>\n",
       "      <td>{'layer_10/width_131k/canonical': 'gemma-2-27b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-att</th>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': 'layer_0/...</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': None, 'la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-att-canonical</th>\n",
       "      <td>gemma-scope-2b-pt-att-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'gemma-2-2b/0-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-mlp</th>\n",
       "      <td>gemma-scope-2b-pt-mlp</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_119': 'layer_0/...</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_119': None, 'la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical</th>\n",
       "      <td>gemma-scope-2b-pt-mlp-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'gemma-2-2b/0-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-res</th>\n",
       "      <td>gemma-scope-2b-pt-res</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'embedding/width_4k/average_l0_6': 'embedding...</td>\n",
       "      <td>{'embedding/width_4k/average_l0_6': None, 'emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-res-canonical</th>\n",
       "      <td>gemma-scope-2b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'gemma-2-2b/0-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-it-res</th>\n",
       "      <td>gemma-scope-9b-it-res</td>\n",
       "      <td>google/gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "      <td>{'layer_20/width_131k/average_l0_13': 'layer_2...</td>\n",
       "      <td>{'layer_20/width_131k/average_l0_13': None, 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-it-res-canonical</th>\n",
       "      <td>gemma-scope-9b-it-res-canonical</td>\n",
       "      <td>google/gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "      <td>{'layer_9/width_16k/canonical': 'layer_9/width...</td>\n",
       "      <td>{'layer_9/width_16k/canonical': 'gemma-2-9b-it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-att</th>\n",
       "      <td>gemma-scope-9b-pt-att</td>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_12': 'layer_0/w...</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_12': None, 'lay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-att-canonical</th>\n",
       "      <td>gemma-scope-9b-pt-att-canonical</td>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'gemma-2-9b/0-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-mlp</th>\n",
       "      <td>gemma-scope-9b-pt-mlp</td>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_6': 'layer_0/wi...</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_6': None, 'laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-mlp-canonical</th>\n",
       "      <td>gemma-scope-9b-pt-mlp-canonical</td>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'gemma-2-9b/0-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-res</th>\n",
       "      <td>gemma-scope-9b-pt-res</td>\n",
       "      <td>google/gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "      <td>{'embedding/width_4k/average_l0_14': 'embeddin...</td>\n",
       "      <td>{'embedding/width_4k/average_l0_14': None, 'em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-res-canonical</th>\n",
       "      <td>gemma-scope-9b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'gemma-2-9b/0-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-attn-out-v5-128k</th>\n",
       "      <td>gpt2-small-attn-out-v5-128k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_attn_out': 'v5_128k_layer_0', ...</td>\n",
       "      <td>{'blocks.0.hook_attn_out': 'gpt2-small/0-att_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-attn-out-v5-32k</th>\n",
       "      <td>gpt2-small-attn-out-v5-32k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_attn_out': 'v5_32k_layer_0', '...</td>\n",
       "      <td>{'blocks.0.hook_attn_out': 'gpt2-small/0-att_3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-hook-z-kk</th>\n",
       "      <td>gpt2-small-hook-z-kk</td>\n",
       "      <td>ckkissane/attn-saes-gpt2-small-all-layers</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_z': 'gpt2-small_L0_Hcat_z_lr1....</td>\n",
       "      <td>{'blocks.0.hook_z': 'gpt2-small/0-att-kk', 'bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-mlp-out-v5-128k</th>\n",
       "      <td>gpt2-small-mlp-out-v5-128k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': 'v5_128k_layer_0', '...</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': 'gpt2-small/0-mlp_12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-mlp-out-v5-32k</th>\n",
       "      <td>gpt2-small-mlp-out-v5-32k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': 'v5_32k_layer_0', 'b...</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': 'gpt2-small/0-mlp_32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-mlp-tm</th>\n",
       "      <td>gpt2-small-mlp-tm</td>\n",
       "      <td>tommmcgrath/gpt2-small-mlp-out-saes</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': 'sae_group_gpt2_bloc...</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': None, 'blocks.1.hook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res-jb</th>\n",
       "      <td>gpt2-small-res-jb</td>\n",
       "      <td>jbloom/GPT2-Small-SAEs-Reformatted</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_pre': 'blocks.0.hook_res...</td>\n",
       "      <td>{'blocks.0.hook_resid_pre': 'gpt2-small/0-res-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res-jb-feature-splitting</th>\n",
       "      <td>gpt2-small-res-jb-feature-splitting</td>\n",
       "      <td>jbloom/GPT2-Small-Feature-Splitting-Experiment...</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.8.hook_resid_pre_768': 'blocks.8.hook...</td>\n",
       "      <td>{'blocks.8.hook_resid_pre_768': 'gpt2-small/8-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_sce-ajt</th>\n",
       "      <td>gpt2-small-res_sce-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_sce-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_sce-ajt', '...</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_scefr-ajt</th>\n",
       "      <td>gpt2-small-res_scefr-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_scefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_scefr-ajt',...</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_scl-ajt</th>\n",
       "      <td>gpt2-small-res_scl-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_scl-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_scl-ajt', '...</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_sle-ajt</th>\n",
       "      <td>gpt2-small-res_sle-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_sle-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_sle-ajt', '...</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_slefr-ajt</th>\n",
       "      <td>gpt2-small-res_slefr-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_slefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_slefr-ajt',...</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_sll-ajt</th>\n",
       "      <td>gpt2-small-res_sll-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_sll-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_sll-ajt', '...</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-resid-mid-v5-128k</th>\n",
       "      <td>gpt2-small-resid-mid-v5-128k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_mid': 'v5_128k_layer_0',...</td>\n",
       "      <td>{'blocks.0.hook_resid_mid': 'gpt2-small/0-res_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-resid-mid-v5-32k</th>\n",
       "      <td>gpt2-small-resid-mid-v5-32k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_mid': 'v5_32k_layer_0', ...</td>\n",
       "      <td>{'blocks.0.hook_resid_mid': 'gpt2-small/0-res_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-resid-post-v5-128k</th>\n",
       "      <td>gpt2-small-resid-post-v5-128k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'v5_128k_layer_0'...</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gpt2-small/0-res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-resid-post-v5-32k</th>\n",
       "      <td>gpt2-small-resid-post-v5-32k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'v5_32k_layer_0.p...</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gpt2-small/0-res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-3-8b-it-res-jh</th>\n",
       "      <td>llama-3-8b-it-res-jh</td>\n",
       "      <td>Juliushanhanhan/llama-3-8b-it-res</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>{'blocks.25.hook_resid_post': 'blocks.25.hook_...</td>\n",
       "      <td>{'blocks.25.hook_resid_post': 'llama3-8b-it/25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-res-wg</th>\n",
       "      <td>mistral-7b-res-wg</td>\n",
       "      <td>JoshEngels/Mistral-7B-Residual-Stream-SAEs</td>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>{'blocks.8.hook_resid_pre': 'mistral_7b_layer_...</td>\n",
       "      <td>{'blocks.8.hook_resid_pre': None, 'blocks.16.h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-70m-deduped-att-sm</th>\n",
       "      <td>pythia-70m-deduped-att-sm</td>\n",
       "      <td>ctigges/pythia-70m-deduped__att-sm_processed</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.0.hook_attn_out': '0-att-sm', 'blocks...</td>\n",
       "      <td>{'blocks.0.hook_attn_out': 'pythia-70m-deduped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-70m-deduped-mlp-sm</th>\n",
       "      <td>pythia-70m-deduped-mlp-sm</td>\n",
       "      <td>ctigges/pythia-70m-deduped__mlp-sm_processed</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': '0-mlp-sm', 'blocks....</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': 'pythia-70m-deduped/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-70m-deduped-res-sm</th>\n",
       "      <td>pythia-70m-deduped-res-sm</td>\n",
       "      <td>ctigges/pythia-70m-deduped__res-sm_processed</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.0.hook_resid_pre': 'e-res-sm', 'block...</td>\n",
       "      <td>{'blocks.0.hook_resid_pre': 'pythia-70m-dedupe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_0824</th>\n",
       "      <td>sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2...</td>\n",
       "      <td>canrager/lm_sae</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_1_step_292...</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_1_step_292...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_0824</th>\n",
       "      <td>sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8...</td>\n",
       "      <td>canrager/lm_sae</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_5_step_488...</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_5_step_488...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824</th>\n",
       "      <td>sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824</td>\n",
       "      <td>canrager/lm_sae</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_2_step_976...</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_2_step_976...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824</th>\n",
       "      <td>sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824</td>\n",
       "      <td>canrager/lm_sae</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_3': 'gemma...</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_3': 'gemma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sae_bench_pythia70m_sweep_gated_ctx128_0730</th>\n",
       "      <td>sae_bench_pythia70m_sweep_gated_ctx128_0730</td>\n",
       "      <td>canrager/lm_sae</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_0': 'pythi...</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_0': 'pythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sae_bench_pythia70m_sweep_panneal_ctx128_0730</th>\n",
       "      <td>sae_bench_pythia70m_sweep_panneal_ctx128_0730</td>\n",
       "      <td>canrager/lm_sae</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_16': 'pyth...</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_16': 'pyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sae_bench_pythia70m_sweep_standard_ctx128_0712</th>\n",
       "      <td>sae_bench_pythia70m_sweep_standard_ctx128_0712</td>\n",
       "      <td>canrager/lm_sae</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_10': 'pyth...</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_10': 'pyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sae_bench_pythia70m_sweep_topk_ctx128_0730</th>\n",
       "      <td>sae_bench_pythia70m_sweep_topk_ctx128_0730</td>\n",
       "      <td>canrager/lm_sae</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_0': 'pythi...</td>\n",
       "      <td>{'blocks.3.hook_resid_post__trainer_0': 'pythi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              release  \\\n",
       "gemma-2b-it-res-jb                                                                 gemma-2b-it-res-jb   \n",
       "gemma-2b-res-jb                                                                       gemma-2b-res-jb   \n",
       "gemma-scope-27b-pt-res                                                         gemma-scope-27b-pt-res   \n",
       "gemma-scope-27b-pt-res-canonical                                     gemma-scope-27b-pt-res-canonical   \n",
       "gemma-scope-2b-pt-att                                                           gemma-scope-2b-pt-att   \n",
       "gemma-scope-2b-pt-att-canonical                                       gemma-scope-2b-pt-att-canonical   \n",
       "gemma-scope-2b-pt-mlp                                                           gemma-scope-2b-pt-mlp   \n",
       "gemma-scope-2b-pt-mlp-canonical                                       gemma-scope-2b-pt-mlp-canonical   \n",
       "gemma-scope-2b-pt-res                                                           gemma-scope-2b-pt-res   \n",
       "gemma-scope-2b-pt-res-canonical                                       gemma-scope-2b-pt-res-canonical   \n",
       "gemma-scope-9b-it-res                                                           gemma-scope-9b-it-res   \n",
       "gemma-scope-9b-it-res-canonical                                       gemma-scope-9b-it-res-canonical   \n",
       "gemma-scope-9b-pt-att                                                           gemma-scope-9b-pt-att   \n",
       "gemma-scope-9b-pt-att-canonical                                       gemma-scope-9b-pt-att-canonical   \n",
       "gemma-scope-9b-pt-mlp                                                           gemma-scope-9b-pt-mlp   \n",
       "gemma-scope-9b-pt-mlp-canonical                                       gemma-scope-9b-pt-mlp-canonical   \n",
       "gemma-scope-9b-pt-res                                                           gemma-scope-9b-pt-res   \n",
       "gemma-scope-9b-pt-res-canonical                                       gemma-scope-9b-pt-res-canonical   \n",
       "gpt2-small-attn-out-v5-128k                                               gpt2-small-attn-out-v5-128k   \n",
       "gpt2-small-attn-out-v5-32k                                                 gpt2-small-attn-out-v5-32k   \n",
       "gpt2-small-hook-z-kk                                                             gpt2-small-hook-z-kk   \n",
       "gpt2-small-mlp-out-v5-128k                                                 gpt2-small-mlp-out-v5-128k   \n",
       "gpt2-small-mlp-out-v5-32k                                                   gpt2-small-mlp-out-v5-32k   \n",
       "gpt2-small-mlp-tm                                                                   gpt2-small-mlp-tm   \n",
       "gpt2-small-res-jb                                                                   gpt2-small-res-jb   \n",
       "gpt2-small-res-jb-feature-splitting                               gpt2-small-res-jb-feature-splitting   \n",
       "gpt2-small-res_sce-ajt                                                         gpt2-small-res_sce-ajt   \n",
       "gpt2-small-res_scefr-ajt                                                     gpt2-small-res_scefr-ajt   \n",
       "gpt2-small-res_scl-ajt                                                         gpt2-small-res_scl-ajt   \n",
       "gpt2-small-res_sle-ajt                                                         gpt2-small-res_sle-ajt   \n",
       "gpt2-small-res_slefr-ajt                                                     gpt2-small-res_slefr-ajt   \n",
       "gpt2-small-res_sll-ajt                                                         gpt2-small-res_sll-ajt   \n",
       "gpt2-small-resid-mid-v5-128k                                             gpt2-small-resid-mid-v5-128k   \n",
       "gpt2-small-resid-mid-v5-32k                                               gpt2-small-resid-mid-v5-32k   \n",
       "gpt2-small-resid-post-v5-128k                                           gpt2-small-resid-post-v5-128k   \n",
       "gpt2-small-resid-post-v5-32k                                             gpt2-small-resid-post-v5-32k   \n",
       "llama-3-8b-it-res-jh                                                             llama-3-8b-it-res-jh   \n",
       "mistral-7b-res-wg                                                                   mistral-7b-res-wg   \n",
       "pythia-70m-deduped-att-sm                                                   pythia-70m-deduped-att-sm   \n",
       "pythia-70m-deduped-mlp-sm                                                   pythia-70m-deduped-mlp-sm   \n",
       "pythia-70m-deduped-res-sm                                                   pythia-70m-deduped-res-sm   \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...  sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2...   \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...  sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8...   \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824       sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824   \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824       sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824   \n",
       "sae_bench_pythia70m_sweep_gated_ctx128_0730               sae_bench_pythia70m_sweep_gated_ctx128_0730   \n",
       "sae_bench_pythia70m_sweep_panneal_ctx128_0730           sae_bench_pythia70m_sweep_panneal_ctx128_0730   \n",
       "sae_bench_pythia70m_sweep_standard_ctx128_0712         sae_bench_pythia70m_sweep_standard_ctx128_0712   \n",
       "sae_bench_pythia70m_sweep_topk_ctx128_0730                 sae_bench_pythia70m_sweep_topk_ctx128_0730   \n",
       "\n",
       "                                                                                              repo_id  \\\n",
       "gemma-2b-it-res-jb                                            jbloom/Gemma-2b-IT-Residual-Stream-SAEs   \n",
       "gemma-2b-res-jb                                                  jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "gemma-scope-27b-pt-res                                                  google/gemma-scope-27b-pt-res   \n",
       "gemma-scope-27b-pt-res-canonical                                        google/gemma-scope-27b-pt-res   \n",
       "gemma-scope-2b-pt-att                                                    google/gemma-scope-2b-pt-att   \n",
       "gemma-scope-2b-pt-att-canonical                                          google/gemma-scope-2b-pt-att   \n",
       "gemma-scope-2b-pt-mlp                                                    google/gemma-scope-2b-pt-mlp   \n",
       "gemma-scope-2b-pt-mlp-canonical                                          google/gemma-scope-2b-pt-mlp   \n",
       "gemma-scope-2b-pt-res                                                    google/gemma-scope-2b-pt-res   \n",
       "gemma-scope-2b-pt-res-canonical                                          google/gemma-scope-2b-pt-res   \n",
       "gemma-scope-9b-it-res                                                    google/gemma-scope-9b-it-res   \n",
       "gemma-scope-9b-it-res-canonical                                          google/gemma-scope-9b-it-res   \n",
       "gemma-scope-9b-pt-att                                                    google/gemma-scope-9b-pt-att   \n",
       "gemma-scope-9b-pt-att-canonical                                          google/gemma-scope-9b-pt-att   \n",
       "gemma-scope-9b-pt-mlp                                                    google/gemma-scope-9b-pt-mlp   \n",
       "gemma-scope-9b-pt-mlp-canonical                                          google/gemma-scope-9b-pt-mlp   \n",
       "gemma-scope-9b-pt-res                                                    google/gemma-scope-9b-pt-res   \n",
       "gemma-scope-9b-pt-res-canonical                                          google/gemma-scope-9b-pt-res   \n",
       "gpt2-small-attn-out-v5-128k                               jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs   \n",
       "gpt2-small-attn-out-v5-32k                                 jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs   \n",
       "gpt2-small-hook-z-kk                                        ckkissane/attn-saes-gpt2-small-all-layers   \n",
       "gpt2-small-mlp-out-v5-128k                                 jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs   \n",
       "gpt2-small-mlp-out-v5-32k                                   jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs   \n",
       "gpt2-small-mlp-tm                                                 tommmcgrath/gpt2-small-mlp-out-saes   \n",
       "gpt2-small-res-jb                                                  jbloom/GPT2-Small-SAEs-Reformatted   \n",
       "gpt2-small-res-jb-feature-splitting                 jbloom/GPT2-Small-Feature-Splitting-Experiment...   \n",
       "gpt2-small-res_sce-ajt                                            neuronpedia/gpt2-small__res_sce-ajt   \n",
       "gpt2-small-res_scefr-ajt                                        neuronpedia/gpt2-small__res_scefr-ajt   \n",
       "gpt2-small-res_scl-ajt                                            neuronpedia/gpt2-small__res_scl-ajt   \n",
       "gpt2-small-res_sle-ajt                                            neuronpedia/gpt2-small__res_sle-ajt   \n",
       "gpt2-small-res_slefr-ajt                                        neuronpedia/gpt2-small__res_slefr-ajt   \n",
       "gpt2-small-res_sll-ajt                                            neuronpedia/gpt2-small__res_sll-ajt   \n",
       "gpt2-small-resid-mid-v5-128k                             jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs   \n",
       "gpt2-small-resid-mid-v5-32k                               jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs   \n",
       "gpt2-small-resid-post-v5-128k                           jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs   \n",
       "gpt2-small-resid-post-v5-32k                             jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs   \n",
       "llama-3-8b-it-res-jh                                                Juliushanhanhan/llama-3-8b-it-res   \n",
       "mistral-7b-res-wg                                          JoshEngels/Mistral-7B-Residual-Stream-SAEs   \n",
       "pythia-70m-deduped-att-sm                                ctigges/pythia-70m-deduped__att-sm_processed   \n",
       "pythia-70m-deduped-mlp-sm                                ctigges/pythia-70m-deduped__mlp-sm_processed   \n",
       "pythia-70m-deduped-res-sm                                ctigges/pythia-70m-deduped__res-sm_processed   \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...                                    canrager/lm_sae   \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...                                    canrager/lm_sae   \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824                                       canrager/lm_sae   \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824                                       canrager/lm_sae   \n",
       "sae_bench_pythia70m_sweep_gated_ctx128_0730                                           canrager/lm_sae   \n",
       "sae_bench_pythia70m_sweep_panneal_ctx128_0730                                         canrager/lm_sae   \n",
       "sae_bench_pythia70m_sweep_standard_ctx128_0712                                        canrager/lm_sae   \n",
       "sae_bench_pythia70m_sweep_topk_ctx128_0730                                            canrager/lm_sae   \n",
       "\n",
       "                                                                                  model  \\\n",
       "gemma-2b-it-res-jb                                                          gemma-2b-it   \n",
       "gemma-2b-res-jb                                                                gemma-2b   \n",
       "gemma-scope-27b-pt-res                                                      gemma-2-27b   \n",
       "gemma-scope-27b-pt-res-canonical                                            gemma-2-27b   \n",
       "gemma-scope-2b-pt-att                                                        gemma-2-2b   \n",
       "gemma-scope-2b-pt-att-canonical                                              gemma-2-2b   \n",
       "gemma-scope-2b-pt-mlp                                                        gemma-2-2b   \n",
       "gemma-scope-2b-pt-mlp-canonical                                              gemma-2-2b   \n",
       "gemma-scope-2b-pt-res                                                        gemma-2-2b   \n",
       "gemma-scope-2b-pt-res-canonical                                              gemma-2-2b   \n",
       "gemma-scope-9b-it-res                                                        gemma-2-9b   \n",
       "gemma-scope-9b-it-res-canonical                                           gemma-2-9b-it   \n",
       "gemma-scope-9b-pt-att                                                        gemma-2-9b   \n",
       "gemma-scope-9b-pt-att-canonical                                              gemma-2-9b   \n",
       "gemma-scope-9b-pt-mlp                                                        gemma-2-9b   \n",
       "gemma-scope-9b-pt-mlp-canonical                                              gemma-2-9b   \n",
       "gemma-scope-9b-pt-res                                                        gemma-2-9b   \n",
       "gemma-scope-9b-pt-res-canonical                                              gemma-2-9b   \n",
       "gpt2-small-attn-out-v5-128k                                                  gpt2-small   \n",
       "gpt2-small-attn-out-v5-32k                                                   gpt2-small   \n",
       "gpt2-small-hook-z-kk                                                         gpt2-small   \n",
       "gpt2-small-mlp-out-v5-128k                                                   gpt2-small   \n",
       "gpt2-small-mlp-out-v5-32k                                                    gpt2-small   \n",
       "gpt2-small-mlp-tm                                                            gpt2-small   \n",
       "gpt2-small-res-jb                                                            gpt2-small   \n",
       "gpt2-small-res-jb-feature-splitting                                          gpt2-small   \n",
       "gpt2-small-res_sce-ajt                                                       gpt2-small   \n",
       "gpt2-small-res_scefr-ajt                                                     gpt2-small   \n",
       "gpt2-small-res_scl-ajt                                                       gpt2-small   \n",
       "gpt2-small-res_sle-ajt                                                       gpt2-small   \n",
       "gpt2-small-res_slefr-ajt                                                     gpt2-small   \n",
       "gpt2-small-res_sll-ajt                                                       gpt2-small   \n",
       "gpt2-small-resid-mid-v5-128k                                                 gpt2-small   \n",
       "gpt2-small-resid-mid-v5-32k                                                  gpt2-small   \n",
       "gpt2-small-resid-post-v5-128k                                                gpt2-small   \n",
       "gpt2-small-resid-post-v5-32k                                                 gpt2-small   \n",
       "llama-3-8b-it-res-jh                                meta-llama/Meta-Llama-3-8B-Instruct   \n",
       "mistral-7b-res-wg                                                            mistral-7b   \n",
       "pythia-70m-deduped-att-sm                                            pythia-70m-deduped   \n",
       "pythia-70m-deduped-mlp-sm                                            pythia-70m-deduped   \n",
       "pythia-70m-deduped-res-sm                                            pythia-70m-deduped   \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...                           gemma-2-2b   \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...                           gemma-2-2b   \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824                              gemma-2-2b   \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824                              gemma-2-2b   \n",
       "sae_bench_pythia70m_sweep_gated_ctx128_0730                          pythia-70m-deduped   \n",
       "sae_bench_pythia70m_sweep_panneal_ctx128_0730                        pythia-70m-deduped   \n",
       "sae_bench_pythia70m_sweep_standard_ctx128_0712                       pythia-70m-deduped   \n",
       "sae_bench_pythia70m_sweep_topk_ctx128_0730                           pythia-70m-deduped   \n",
       "\n",
       "                                                                                             saes_map  \\\n",
       "gemma-2b-it-res-jb                                  {'blocks.12.hook_resid_post': 'gemma_2b_it_blo...   \n",
       "gemma-2b-res-jb                                     {'blocks.0.hook_resid_post': 'gemma_2b_blocks....   \n",
       "gemma-scope-27b-pt-res                              {'layer_10/width_131k/average_l0_106': 'layer_...   \n",
       "gemma-scope-27b-pt-res-canonical                    {'layer_10/width_131k/canonical': 'layer_10/wi...   \n",
       "gemma-scope-2b-pt-att                               {'layer_0/width_16k/average_l0_104': 'layer_0/...   \n",
       "gemma-scope-2b-pt-att-canonical                     {'layer_0/width_16k/canonical': 'layer_0/width...   \n",
       "gemma-scope-2b-pt-mlp                               {'layer_0/width_16k/average_l0_119': 'layer_0/...   \n",
       "gemma-scope-2b-pt-mlp-canonical                     {'layer_0/width_16k/canonical': 'layer_0/width...   \n",
       "gemma-scope-2b-pt-res                               {'embedding/width_4k/average_l0_6': 'embedding...   \n",
       "gemma-scope-2b-pt-res-canonical                     {'layer_0/width_16k/canonical': 'layer_0/width...   \n",
       "gemma-scope-9b-it-res                               {'layer_20/width_131k/average_l0_13': 'layer_2...   \n",
       "gemma-scope-9b-it-res-canonical                     {'layer_9/width_16k/canonical': 'layer_9/width...   \n",
       "gemma-scope-9b-pt-att                               {'layer_0/width_16k/average_l0_12': 'layer_0/w...   \n",
       "gemma-scope-9b-pt-att-canonical                     {'layer_0/width_16k/canonical': 'layer_0/width...   \n",
       "gemma-scope-9b-pt-mlp                               {'layer_0/width_16k/average_l0_6': 'layer_0/wi...   \n",
       "gemma-scope-9b-pt-mlp-canonical                     {'layer_0/width_16k/canonical': 'layer_0/width...   \n",
       "gemma-scope-9b-pt-res                               {'embedding/width_4k/average_l0_14': 'embeddin...   \n",
       "gemma-scope-9b-pt-res-canonical                     {'layer_0/width_16k/canonical': 'layer_0/width...   \n",
       "gpt2-small-attn-out-v5-128k                         {'blocks.0.hook_attn_out': 'v5_128k_layer_0', ...   \n",
       "gpt2-small-attn-out-v5-32k                          {'blocks.0.hook_attn_out': 'v5_32k_layer_0', '...   \n",
       "gpt2-small-hook-z-kk                                {'blocks.0.hook_z': 'gpt2-small_L0_Hcat_z_lr1....   \n",
       "gpt2-small-mlp-out-v5-128k                          {'blocks.0.hook_mlp_out': 'v5_128k_layer_0', '...   \n",
       "gpt2-small-mlp-out-v5-32k                           {'blocks.0.hook_mlp_out': 'v5_32k_layer_0', 'b...   \n",
       "gpt2-small-mlp-tm                                   {'blocks.0.hook_mlp_out': 'sae_group_gpt2_bloc...   \n",
       "gpt2-small-res-jb                                   {'blocks.0.hook_resid_pre': 'blocks.0.hook_res...   \n",
       "gpt2-small-res-jb-feature-splitting                 {'blocks.8.hook_resid_pre_768': 'blocks.8.hook...   \n",
       "gpt2-small-res_sce-ajt                              {'blocks.2.hook_resid_pre': '2-res_sce-ajt', '...   \n",
       "gpt2-small-res_scefr-ajt                            {'blocks.2.hook_resid_pre': '2-res_scefr-ajt',...   \n",
       "gpt2-small-res_scl-ajt                              {'blocks.2.hook_resid_pre': '2-res_scl-ajt', '...   \n",
       "gpt2-small-res_sle-ajt                              {'blocks.2.hook_resid_pre': '2-res_sle-ajt', '...   \n",
       "gpt2-small-res_slefr-ajt                            {'blocks.2.hook_resid_pre': '2-res_slefr-ajt',...   \n",
       "gpt2-small-res_sll-ajt                              {'blocks.2.hook_resid_pre': '2-res_sll-ajt', '...   \n",
       "gpt2-small-resid-mid-v5-128k                        {'blocks.0.hook_resid_mid': 'v5_128k_layer_0',...   \n",
       "gpt2-small-resid-mid-v5-32k                         {'blocks.0.hook_resid_mid': 'v5_32k_layer_0', ...   \n",
       "gpt2-small-resid-post-v5-128k                       {'blocks.0.hook_resid_post': 'v5_128k_layer_0'...   \n",
       "gpt2-small-resid-post-v5-32k                        {'blocks.0.hook_resid_post': 'v5_32k_layer_0.p...   \n",
       "llama-3-8b-it-res-jh                                {'blocks.25.hook_resid_post': 'blocks.25.hook_...   \n",
       "mistral-7b-res-wg                                   {'blocks.8.hook_resid_pre': 'mistral_7b_layer_...   \n",
       "pythia-70m-deduped-att-sm                           {'blocks.0.hook_attn_out': '0-att-sm', 'blocks...   \n",
       "pythia-70m-deduped-mlp-sm                           {'blocks.0.hook_mlp_out': '0-mlp-sm', 'blocks....   \n",
       "pythia-70m-deduped-res-sm                           {'blocks.0.hook_resid_pre': 'e-res-sm', 'block...   \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...  {'blocks.3.hook_resid_post__trainer_1_step_292...   \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...  {'blocks.3.hook_resid_post__trainer_5_step_488...   \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824     {'blocks.3.hook_resid_post__trainer_2_step_976...   \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824     {'blocks.3.hook_resid_post__trainer_3': 'gemma...   \n",
       "sae_bench_pythia70m_sweep_gated_ctx128_0730         {'blocks.3.hook_resid_post__trainer_0': 'pythi...   \n",
       "sae_bench_pythia70m_sweep_panneal_ctx128_0730       {'blocks.3.hook_resid_post__trainer_16': 'pyth...   \n",
       "sae_bench_pythia70m_sweep_standard_ctx128_0712      {'blocks.3.hook_resid_post__trainer_10': 'pyth...   \n",
       "sae_bench_pythia70m_sweep_topk_ctx128_0730          {'blocks.3.hook_resid_post__trainer_0': 'pythi...   \n",
       "\n",
       "                                                                                       neuronpedia_id  \n",
       "gemma-2b-it-res-jb                                  {'blocks.12.hook_resid_post': 'gemma-2b-it/12-...  \n",
       "gemma-2b-res-jb                                     {'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...  \n",
       "gemma-scope-27b-pt-res                              {'layer_10/width_131k/average_l0_106': None, '...  \n",
       "gemma-scope-27b-pt-res-canonical                    {'layer_10/width_131k/canonical': 'gemma-2-27b...  \n",
       "gemma-scope-2b-pt-att                               {'layer_0/width_16k/average_l0_104': None, 'la...  \n",
       "gemma-scope-2b-pt-att-canonical                     {'layer_0/width_16k/canonical': 'gemma-2-2b/0-...  \n",
       "gemma-scope-2b-pt-mlp                               {'layer_0/width_16k/average_l0_119': None, 'la...  \n",
       "gemma-scope-2b-pt-mlp-canonical                     {'layer_0/width_16k/canonical': 'gemma-2-2b/0-...  \n",
       "gemma-scope-2b-pt-res                               {'embedding/width_4k/average_l0_6': None, 'emb...  \n",
       "gemma-scope-2b-pt-res-canonical                     {'layer_0/width_16k/canonical': 'gemma-2-2b/0-...  \n",
       "gemma-scope-9b-it-res                               {'layer_20/width_131k/average_l0_13': None, 'l...  \n",
       "gemma-scope-9b-it-res-canonical                     {'layer_9/width_16k/canonical': 'gemma-2-9b-it...  \n",
       "gemma-scope-9b-pt-att                               {'layer_0/width_16k/average_l0_12': None, 'lay...  \n",
       "gemma-scope-9b-pt-att-canonical                     {'layer_0/width_16k/canonical': 'gemma-2-9b/0-...  \n",
       "gemma-scope-9b-pt-mlp                               {'layer_0/width_16k/average_l0_6': None, 'laye...  \n",
       "gemma-scope-9b-pt-mlp-canonical                     {'layer_0/width_16k/canonical': 'gemma-2-9b/0-...  \n",
       "gemma-scope-9b-pt-res                               {'embedding/width_4k/average_l0_14': None, 'em...  \n",
       "gemma-scope-9b-pt-res-canonical                     {'layer_0/width_16k/canonical': 'gemma-2-9b/0-...  \n",
       "gpt2-small-attn-out-v5-128k                         {'blocks.0.hook_attn_out': 'gpt2-small/0-att_1...  \n",
       "gpt2-small-attn-out-v5-32k                          {'blocks.0.hook_attn_out': 'gpt2-small/0-att_3...  \n",
       "gpt2-small-hook-z-kk                                {'blocks.0.hook_z': 'gpt2-small/0-att-kk', 'bl...  \n",
       "gpt2-small-mlp-out-v5-128k                          {'blocks.0.hook_mlp_out': 'gpt2-small/0-mlp_12...  \n",
       "gpt2-small-mlp-out-v5-32k                           {'blocks.0.hook_mlp_out': 'gpt2-small/0-mlp_32...  \n",
       "gpt2-small-mlp-tm                                   {'blocks.0.hook_mlp_out': None, 'blocks.1.hook...  \n",
       "gpt2-small-res-jb                                   {'blocks.0.hook_resid_pre': 'gpt2-small/0-res-...  \n",
       "gpt2-small-res-jb-feature-splitting                 {'blocks.8.hook_resid_pre_768': 'gpt2-small/8-...  \n",
       "gpt2-small-res_sce-ajt                              {'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...  \n",
       "gpt2-small-res_scefr-ajt                            {'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...  \n",
       "gpt2-small-res_scl-ajt                              {'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...  \n",
       "gpt2-small-res_sle-ajt                              {'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...  \n",
       "gpt2-small-res_slefr-ajt                            {'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...  \n",
       "gpt2-small-res_sll-ajt                              {'blocks.2.hook_resid_pre': 'gpt2-small/2-res_...  \n",
       "gpt2-small-resid-mid-v5-128k                        {'blocks.0.hook_resid_mid': 'gpt2-small/0-res_...  \n",
       "gpt2-small-resid-mid-v5-32k                         {'blocks.0.hook_resid_mid': 'gpt2-small/0-res_...  \n",
       "gpt2-small-resid-post-v5-128k                       {'blocks.0.hook_resid_post': 'gpt2-small/0-res...  \n",
       "gpt2-small-resid-post-v5-32k                        {'blocks.0.hook_resid_post': 'gpt2-small/0-res...  \n",
       "llama-3-8b-it-res-jh                                {'blocks.25.hook_resid_post': 'llama3-8b-it/25...  \n",
       "mistral-7b-res-wg                                   {'blocks.8.hook_resid_pre': None, 'blocks.16.h...  \n",
       "pythia-70m-deduped-att-sm                           {'blocks.0.hook_attn_out': 'pythia-70m-deduped...  \n",
       "pythia-70m-deduped-mlp-sm                           {'blocks.0.hook_mlp_out': 'pythia-70m-deduped/...  \n",
       "pythia-70m-deduped-res-sm                           {'blocks.0.hook_resid_pre': 'pythia-70m-dedupe...  \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...  {'blocks.3.hook_resid_post__trainer_1_step_292...  \n",
       "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...  {'blocks.3.hook_resid_post__trainer_5_step_488...  \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824     {'blocks.3.hook_resid_post__trainer_2_step_976...  \n",
       "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824     {'blocks.3.hook_resid_post__trainer_3': 'gemma...  \n",
       "sae_bench_pythia70m_sweep_gated_ctx128_0730         {'blocks.3.hook_resid_post__trainer_0': 'pythi...  \n",
       "sae_bench_pythia70m_sweep_panneal_ctx128_0730       {'blocks.3.hook_resid_post__trainer_16': 'pyth...  \n",
       "sae_bench_pythia70m_sweep_standard_ctx128_0712      {'blocks.3.hook_resid_post__trainer_10': 'pyth...  \n",
       "sae_bench_pythia70m_sweep_topk_ctx128_0730          {'blocks.3.hook_resid_post__trainer_0': 'pythi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd6287e6-45ea-45e5-96ed-6e2ec82593dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAEs in the gemma-scope-9b-it-res\n",
      "SAE id: layer_20/width_131k/average_l0_13 for hook point: layer_20/width_131k/average_l0_13\n",
      "SAE id: layer_20/width_131k/average_l0_153 for hook point: layer_20/width_131k/average_l0_153\n",
      "SAE id: layer_20/width_131k/average_l0_24 for hook point: layer_20/width_131k/average_l0_24\n",
      "SAE id: layer_20/width_131k/average_l0_43 for hook point: layer_20/width_131k/average_l0_43\n",
      "SAE id: layer_20/width_131k/average_l0_81 for hook point: layer_20/width_131k/average_l0_81\n",
      "SAE id: layer_20/width_16k/average_l0_14 for hook point: layer_20/width_16k/average_l0_14\n",
      "SAE id: layer_20/width_16k/average_l0_189 for hook point: layer_20/width_16k/average_l0_189\n",
      "SAE id: layer_20/width_16k/average_l0_25 for hook point: layer_20/width_16k/average_l0_25\n",
      "SAE id: layer_20/width_16k/average_l0_47 for hook point: layer_20/width_16k/average_l0_47\n",
      "SAE id: layer_20/width_16k/average_l0_91 for hook point: layer_20/width_16k/average_l0_91\n",
      "SAE id: layer_31/width_131k/average_l0_109 for hook point: layer_31/width_131k/average_l0_109\n",
      "SAE id: layer_31/width_131k/average_l0_13 for hook point: layer_31/width_131k/average_l0_13\n",
      "SAE id: layer_31/width_131k/average_l0_22 for hook point: layer_31/width_131k/average_l0_22\n",
      "SAE id: layer_31/width_131k/average_l0_37 for hook point: layer_31/width_131k/average_l0_37\n",
      "SAE id: layer_31/width_131k/average_l0_63 for hook point: layer_31/width_131k/average_l0_63\n",
      "SAE id: layer_31/width_16k/average_l0_14 for hook point: layer_31/width_16k/average_l0_14\n",
      "SAE id: layer_31/width_16k/average_l0_142 for hook point: layer_31/width_16k/average_l0_142\n",
      "SAE id: layer_31/width_16k/average_l0_24 for hook point: layer_31/width_16k/average_l0_24\n",
      "SAE id: layer_31/width_16k/average_l0_43 for hook point: layer_31/width_16k/average_l0_43\n",
      "SAE id: layer_31/width_16k/average_l0_76 for hook point: layer_31/width_16k/average_l0_76\n",
      "SAE id: layer_9/width_131k/average_l0_121 for hook point: layer_9/width_131k/average_l0_121\n",
      "SAE id: layer_9/width_131k/average_l0_13 for hook point: layer_9/width_131k/average_l0_13\n",
      "SAE id: layer_9/width_131k/average_l0_22 for hook point: layer_9/width_131k/average_l0_22\n",
      "SAE id: layer_9/width_131k/average_l0_39 for hook point: layer_9/width_131k/average_l0_39\n",
      "SAE id: layer_9/width_131k/average_l0_67 for hook point: layer_9/width_131k/average_l0_67\n",
      "SAE id: layer_9/width_16k/average_l0_14 for hook point: layer_9/width_16k/average_l0_14\n",
      "SAE id: layer_9/width_16k/average_l0_186 for hook point: layer_9/width_16k/average_l0_186\n",
      "SAE id: layer_9/width_16k/average_l0_26 for hook point: layer_9/width_16k/average_l0_26\n",
      "SAE id: layer_9/width_16k/average_l0_47 for hook point: layer_9/width_16k/average_l0_47\n",
      "SAE id: layer_9/width_16k/average_l0_88 for hook point: layer_9/width_16k/average_l0_88\n"
     ]
    }
   ],
   "source": [
    "print(f\"SAEs in the {SAE_ID}\")\n",
    "for k, v in df.loc[df.release==SAE_ID,\"saes_map\"].values[0].items():print(f\"SAE id: {k} for hook point: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252631e-e1b5-4fd7-86e0-73d6b62a301f",
   "metadata": {},
   "source": [
    "---\n",
    "## models\n",
    "\n",
    "We will be using [GemmaScope](https://huggingface.co/google/gemma-scope-9b-pt-res/tree/main) and [google/gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it) for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "442dd8cc-520b-49ac-a31a-93880536ba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1310d12cc18f41dba4bd1d17e8974cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Make sure to only run this once\n",
    "model = HookedSAETransformer.from_pretrained(MODEL, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36ec477-b481-4c5c-9dec-ab4664072495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=SAE_ID, device=device,\n",
    "    sae_id=f\"layer_{layer}/width_131k/average_l0_81\", # test with L0_81\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f661df4-306b-4853-9d54-1395f6c7399b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'jumprelu',\n",
       " 'd_in': 3584,\n",
       " 'd_sae': 131072,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'context_size': 1024,\n",
       " 'model_name': 'gemma-2-9b-it',\n",
       " 'hook_name': 'blocks.20.hook_resid_post',\n",
       " 'hook_layer': 20,\n",
       " 'hook_head_index': None,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_path': 'monology/pile-uncopyrighted',\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'normalize_activations': None,\n",
       " 'dtype': 'float32',\n",
       " 'device': 'cuda',\n",
       " 'sae_lens_training_version': None,\n",
       " 'activation_fn_kwargs': {},\n",
       " 'neuronpedia_id': None,\n",
       " 'model_from_pretrained_kwargs': {},\n",
       " 'seqpos_slice': (None,)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get hook point\n",
    "hook_point = sae.cfg.hook_name\n",
    "sae.cfg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3488bcc-3c24-4123-94cd-2b1b8bd660e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "dataset = load_dataset(path=\"NeelNanda/pile-10k\", split=\"train\", streaming=False)\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset, tokenizer=model.tokenizer,\n",
    "    streaming=True,\n",
    "    max_length=sae.cfg.context_size,\n",
    "    add_bos_token=sae.cfg.prepend_bos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14517c85-4faf-4f1b-a6e9-40e82722a383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,    651,  92276,   1129,  12671,    476,   2619,   5368,    774,\n",
      "            573,   4701,    576,    573,   3609, 235265,   1315,   1093, 112935,\n",
      "            573,  64145,   1280, 137007,    674,   5764, 235269,    948,   1049,\n",
      "            575,    573,   5027,   2965, 235265]], device='cuda:0')\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[3156.4102,  774.5602,  577.5037],\n",
      "         [1156.9087,  405.6550,  163.4940],\n",
      "         [  35.3094,   31.4556,   30.7801],\n",
      "         [  33.7741,   30.0334,   29.1854],\n",
      "         [  44.0230,   37.2889,   32.9577],\n",
      "         [  37.9170,   32.5813,   28.3535],\n",
      "         [  70.3743,   39.1346,   29.2105],\n",
      "         [  42.5608,   31.9773,   30.9554],\n",
      "         [  44.6310,   35.2725,   33.9535],\n",
      "         [  36.2088,   23.6369,   18.6098],\n",
      "         [  47.2001,   35.4091,   30.4748],\n",
      "         [  40.7974,   39.5112,   35.0373],\n",
      "         [  47.3596,   30.3908,   26.2560],\n",
      "         [  68.0637,   63.9519,   50.8768],\n",
      "         [  53.7122,   39.3916,   30.7550],\n",
      "         [  49.3723,   34.9850,   31.5115],\n",
      "         [  36.5596,   32.4357,   23.2460],\n",
      "         [  44.6627,   35.5239,   26.2068],\n",
      "         [  40.3446,   34.0830,   26.1196],\n",
      "         [  78.2485,   43.9478,   33.2486],\n",
      "         [  46.7238,   30.7397,   28.7286],\n",
      "         [  56.0803,   36.5313,   32.9038],\n",
      "         [  52.4517,   37.1640,   19.1627],\n",
      "         [  59.5672,   46.7323,   38.0720],\n",
      "         [  57.5405,   42.0705,   37.4063],\n",
      "         [  45.9480,   40.3062,   33.2769],\n",
      "         [  35.4304,   33.0563,   25.4625],\n",
      "         [  50.5602,   44.6627,   24.9766],\n",
      "         [  29.6167,   27.2940,   24.3107],\n",
      "         [  58.7017,   42.8358,   27.2849],\n",
      "         [  62.6868,   47.7491,   33.1161],\n",
      "         [  56.9456,   34.2453,   20.8972]]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[[ 90871,  53897,  97369],\n",
      "         [ 90871,  74838,  52094],\n",
      "         [ 80145,  92374,  32283],\n",
      "         [ 90871,  43499,  90815],\n",
      "         [ 90871,  43499,  32283],\n",
      "         [ 51811,  97342,  32283],\n",
      "         [109746,  51811,  32283],\n",
      "         [ 32471,  32283,  32687],\n",
      "         [114503, 110523,  85645],\n",
      "         [ 64753,  32283,  20048],\n",
      "         [ 40063,  51811, 124311],\n",
      "         [ 64836,  51811, 124311],\n",
      "         [ 90871,  64753,  26379],\n",
      "         [124311,  90871,  94454],\n",
      "         [122357,  32283,  85645],\n",
      "         [ 90871,  32283,  28791],\n",
      "         [ 96720,  32283,  79565],\n",
      "         [ 49176,  32283,  81709],\n",
      "         [ 64753,  51811,  32283],\n",
      "         [ 69133,  90871,  32283],\n",
      "         [ 86311,  32283,  28032],\n",
      "         [ 90871, 113234, 106631],\n",
      "         [   715,  32283,  78755],\n",
      "         [ 95300,  90871,  32283],\n",
      "         [ 88722,  90871,  32283],\n",
      "         [ 51811, 116403,  64458],\n",
      "         [ 62915,   9766,  41337],\n",
      "         [ 51811,  61178, 103705],\n",
      "         [ 32283,  64753,  43499],\n",
      "         [ 90871,  28342,  84677],\n",
      "         [ 90871,  84677,  27610],\n",
      "         [122357,  32283,  44011]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "sv_prompt=\"\"\"The chiffonier stood a few feet from the foot of the bed. He had emptied the drawers into cartons that morning, which were in the living room.\"\"\"\n",
    "\n",
    "sv_logits, cache = model.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "tokens = model.to_tokens(sv_prompt)\n",
    "print(tokens)\n",
    "\n",
    "# get the feature activations from our SAE\n",
    "sv_feature_acts = sae.encode(cache[hook_point])\n",
    "\n",
    "# get sae_out\n",
    "sae_out = sae.decode(sv_feature_acts)\n",
    "\n",
    "# print out the top activations, focus on the indices\n",
    "print(torch.topk(sv_feature_acts, 3))\n",
    "top_indices = torch.topk(sv_feature_acts, 3).indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eaa4441-5e88-4084-a46d-3353291b96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.cfg.neuronpedia_id = MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22342bee-c6e3-4832-9018-bebe98760d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neuronpedia.org/gemma-2-9b-it/20-gemmascope-res-131k/43499?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-9b-it/20-gemmascope-res-131k/43499?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f5e261a6d50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame, HTML\n",
    "\n",
    "# get a random feature from the SAE\n",
    "feature_idx = torch.randint(0, sae.cfg.d_sae, (1,)).item()\n",
    "\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "\n",
    "def get_dashboard_html(sae_release=\"gemma-2-9b-it\", sae_id=f\"{layer}-gemmascope-res-131k\", feature_idx=0):\n",
    "    print((result := html_template.format(sae_release, sae_id, feature_idx)))\n",
    "    return result\n",
    "\n",
    "\n",
    "html = get_dashboard_html(feature_idx=43499)\n",
    "IFrame(html, width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bfb507e-3f26-42f9-8c2f-3afd97505fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_activation(model, sae, activation_store, feature_idx, num_batches=100):\n",
    "    \"\"\"\n",
    "    Find the maximum activation for a given feature index. This is useful for\n",
    "    calibrating the right amount of the feature to add.\n",
    "    \"\"\"\n",
    "    max_activation = 0.0\n",
    "\n",
    "    pbar = tqdm(range(num_batches))\n",
    "    for _ in pbar:\n",
    "        tokens = activation_store.get_batch_tokens()\n",
    "\n",
    "        _, cache = model.run_with_cache(\n",
    "            tokens,\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae.cfg.hook_name],\n",
    "        )\n",
    "        sae_in = cache[sae.cfg.hook_name]\n",
    "        feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "        feature_acts = feature_acts.flatten(0, 1)\n",
    "        batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
    "        max_activation = max(max_activation, batch_max_activation)\n",
    "\n",
    "        pbar.set_description(f\"Max activation: {max_activation:.4f}\")\n",
    "\n",
    "    return max_activation\n",
    "\n",
    "\n",
    "def steering(activations, hook, steering_strength=1.0, steering_vector=None, max_act=1.0):\n",
    "    return activations + max_act * steering_strength * steering_vector\n",
    "\n",
    "\n",
    "def generate_with_steering(\n",
    "    model,\n",
    "    sae,\n",
    "    prompt,\n",
    "    steering_feature,\n",
    "    max_act,\n",
    "    steering_strength=1.0,\n",
    "    max_new_tokens=95,\n",
    "):\n",
    "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "    steering_vector = sae.W_dec[steering_feature].to(model.cfg.device)\n",
    "\n",
    "    steering_hook = partial(\n",
    "        steering,\n",
    "        steering_vector=steering_vector,\n",
    "        steering_strength=steering_strength,\n",
    "        max_act=max_act,\n",
    "    )\n",
    "\n",
    "    # standard transformerlens syntax for a hook context for generation\n",
    "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=False if device == \"mps\" else True,\n",
    "            prepend_bos=sae.cfg.prepend_bos,\n",
    "        )\n",
    "\n",
    "    full_text = model.tokenizer.decode(output[0])\n",
    "    \n",
    "    # Decode the prompt and generated text separately\n",
    "    prompt_len = len(input_ids[0])\n",
    "    prompt_tokens = output[0][:prompt_len]\n",
    "    generated_tokens = output[0][prompt_len:]\n",
    "    \n",
    "    prompt_text = model.tokenizer.decode(prompt_tokens)\n",
    "    generated_text = model.tokenizer.decode(generated_tokens)\n",
    "    \n",
    "    # Create HTML with different colors\n",
    "    html_output = f\"\"\"\n",
    "    <div style=\"font-family: monospace;\">\n",
    "        <span style=\"color: black;\">{prompt_text}</span>\n",
    "        <span style=\"color: blue; font-weight: bold;\">{generated_text}</span>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the colored output\n",
    "    display(HTML(html_output))\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53cb99bb-8b45-4a28-9bb8-a2b42eee1e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum activation for feature 43499: 50.6600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c42698365eb4f72a1cb63edae530036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normal text (without steering):\n",
      "The chiffonier stood a few feet from the foot of the bed. He had emptied the drawers into cartons that morning, which were in the living room.  He stood for a moment, staring at the empty space where the bristly monster of wood and fabric had housed all the ghosts of his past.\n",
      "\n",
      "He closed his eyes, trying to recall the feeling of opening the top drawer. The smell of dust and cedarwood always hit him first, followed by the whisper of paper and the faint scent of lavender, a vapor trail from the hand-painted brooch his grandmother had sewn against a faded silk dress. His grandmother. His\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff8d07afa7b4d40913b0c92faab5078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"font-family: monospace;\">\n",
       "        <span style=\"color: black;\"><bos>The chiffonier stood a few feet from the foot of the bed. He had emptied the drawers into cartons that morning, which were in the living room.</span>\n",
       "        <span style=\"color: blue; font-weight: bold;\"> Now, all that remained was the chiffonier itself, a solid piece of mahogany with intricate carvings and brass handles.\n",
       "\n",
       "The man stood in the middle of the room, looking at it with a mixture of fondness and regret. It had been his grandfather's, a piece of furniture that had seen generations come and go.\n",
       "\n",
       "\"It's too big,\" he sighed.\n",
       "\n",
       "The room was small, the walls close. The chiffonier, with its imposing presence,</span>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered text:\n",
      "<bos>The chiffonier stood a few feet from the foot of the bed. He had emptied the drawers into cartons that morning, which were in the living room. Now, all that remained was the chiffonier itself, a solid piece of mahogany with intricate carvings and brass handles.\n",
      "\n",
      "The man stood in the middle of the room, looking at it with a mixture of fondness and regret. It had been his grandfather's, a piece of furniture that had seen generations come and go.\n",
      "\n",
      "\"It's too big,\" he sighed.\n",
      "\n",
      "The room was small, the walls close. The chiffonier, with its imposing presence,\n"
     ]
    }
   ],
   "source": [
    "# Choose a feature to steer\n",
    "steering_feature = steering_feature = 43499\n",
    "\n",
    "# Find the maximum activation for this feature\n",
    "# NOTE: we could also get the max activation from Neuronpedia (https://www.neuronpedia.org/api-doc#tag/lookup/GET/api/feature/{modelId}/{layer}/{index})\n",
    "max_act = 50.66\n",
    "print(f\"Maximum activation for feature {steering_feature}: {max_act:.4f}\")\n",
    "\n",
    "\n",
    "# Generate text without steering for comparison\n",
    "normal_text = model.generate(\n",
    "    sv_prompt,\n",
    "    max_new_tokens=95,\n",
    "    stop_at_eos=False if device == \"mps\" else True,\n",
    "    prepend_bos=sae.cfg.prepend_bos,\n",
    ")\n",
    "\n",
    "print(\"\\nNormal text (without steering):\")\n",
    "print(normal_text)\n",
    "\n",
    "# Generate text with steering\n",
    "steered_text = generate_with_steering(\n",
    "    model, sae, sv_prompt, steering_feature, max_act, steering_strength=2.0\n",
    ")\n",
    "print(\"Steered text:\")\n",
    "print(steered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143d5c1-80c4-4939-afac-b9cd03706c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9209b01-09b1-4214-91c0-4c879fd1f304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491c9cc-0ad3-4479-b516-6502d2f73cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
