{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9303e3-41b4-4490-a9d6-b6f694527b9d",
   "metadata": {},
   "source": [
    "---\n",
    "## general setup (don't bother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deef11f-7bf4-4755-a561-7adb67a5c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install --upgrade sae-lens transformer-lens sae-dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80461223-b29d-46d5-8be9-071d3cd01ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Int, Float\n",
    "\n",
    "import torch, pathlib, pandas as pd\n",
    "import huggingface_hub as hf_hub, safetensors as st\n",
    "\n",
    "# device setup\n",
    "if torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEBUG=True\n",
    "ONCE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6fa02-881f-4c9b-b10d-44f1d76057cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "from datasets import load_dataset\n",
    "from transformer_lens.utils import tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c9773-e4c6-418c-b383-5c1170ba165e",
   "metadata": {},
   "source": [
    "---\n",
    "## metadata for SAE exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b32d3-93a4-4428-baf9-5ea58c91c119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records({k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}).T\n",
    "df.drop(columns=['expected_var_explained', 'expected_l0', 'config_overrides', 'conversion_func'], inplace=True)\n",
    "\n",
    "# currently only layer 6 works pretty well\n",
    "layer = 20\n",
    "MODEL = 'google/gemma-2-9b-it'\n",
    "SAE_ID = 'gemma-scope-9b-it-res'\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6287e6-45ea-45e5-96ed-6e2ec82593dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "  print(f'SAEs in the {SAE_ID}')\n",
    "  for k, v in df.loc[df.release == SAE_ID, 'saes_map'].values[0].items():print(f'SAE id: {k} for hook point: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252631e-e1b5-4fd7-86e0-73d6b62a301f",
   "metadata": {},
   "source": [
    "---\n",
    "## models\n",
    "\n",
    "We will be using [GemmaScope](https://huggingface.co/google/gemma-scope-9b-pt-res/tree/main) and [google/gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it) for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dd8cc-520b-49ac-a31a-93880536ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if not ONCE:model = HookedSAETransformer.from_pretrained(MODEL, device=device); ONCE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ec477-b481-4c5c-9dec-ab4664072495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "  release=SAE_ID, device=device,\n",
    "  sae_id=f'layer_{layer}/width_131k/average_l0_81',  # test with L0_81\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f661df4-306b-4853-9d54-1395f6c7399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hook point\n",
    "hook_point = sae.cfg.hook_name\n",
    "\n",
    "# setup some cfg\n",
    "sae.cfg.neuronpedia_id = MODEL\n",
    "\n",
    "# print out dict\n",
    "sae.cfg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3488bcc-3c24-4123-94cd-2b1b8bd660e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(path='NeelNanda/pile-10k', split='train', streaming=False)\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "  dataset=dataset,\n",
    "  tokenizer=model.tokenizer,\n",
    "  streaming=True,\n",
    "  max_length=sae.cfg.context_size,\n",
    "  add_bos_token=sae.cfg.prepend_bos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14517c85-4faf-4f1b-a6e9-40e82722a383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sv_prompt = \"\"\"The chiffonier stood a few feet from the foot of the bed. He had emptied the drawers into cartons that morning, which were in the living room.\"\"\"\n",
    "\n",
    "sv_logits, cache = model.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "tokens = model.to_tokens(sv_prompt)\n",
    "print(tokens)\n",
    "\n",
    "# get the feature activations from our SAE\n",
    "sv_feature_acts = sae.encode(cache[hook_point])\n",
    "\n",
    "# get sae_out\n",
    "sae_out = sae.decode(sv_feature_acts)\n",
    "\n",
    "# print out the top activations, focus on the indices\n",
    "print(torch.topk(sv_feature_acts, 3))\n",
    "top_indices = torch.topk(sv_feature_acts, 3).indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22342bee-c6e3-4832-9018-bebe98760d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, HTML\n",
    "\n",
    "# get a random feature from the SAE\n",
    "feature_idx = torch.randint(0, sae.cfg.d_sae, (1,)).item()\n",
    "\n",
    "html_template = 'https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true'\n",
    "\n",
    "def get_dashboard_html(sae_release='gemma-2-9b-it', sae_id=f'{layer}-gemmascope-res-131k', feature_idx=0):\n",
    "  print((result := html_template.format(sae_release, sae_id, feature_idx)))\n",
    "  return result\n",
    "\n",
    "\n",
    "html = get_dashboard_html(feature_idx=43499)\n",
    "IFrame(html, width=1800, height=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08534f11-9e27-436b-b8b9-f8c5fe56938e",
   "metadata": {},
   "source": [
    "---\n",
    "## steering logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb507e-3f26-42f9-8c2f-3afd97505fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_activation(model, sae, activation_store, feature_idx, num_batches=100):\n",
    "  \"\"\"\n",
    "  Find the maximum activation for a given feature index. This is useful for\n",
    "  calibrating the right amount of the feature to add.\n",
    "  \"\"\"\n",
    "  max_activation = 0.0\n",
    "\n",
    "  pbar = tqdm(range(num_batches))\n",
    "  for _ in pbar:\n",
    "    tokens = activation_store.get_batch_tokens()\n",
    "\n",
    "    _, cache = model.run_with_cache(tokens, stop_at_layer=sae.cfg.hook_layer + 1, names_filter=[sae.cfg.hook_name])\n",
    "    sae_in = cache[sae.cfg.hook_name]\n",
    "    feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "    feature_acts = feature_acts.flatten(0, 1)\n",
    "    batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
    "    max_activation = max(max_activation, batch_max_activation)\n",
    "\n",
    "    pbar.set_description(f'Max activation: {max_activation:.4f}')\n",
    "\n",
    "  return max_activation\n",
    "\n",
    "\n",
    "def steering(activations, hook, steering_strength=1.0, steering_vector=None, max_act=1.0):\n",
    "  return activations + max_act * steering_strength * steering_vector\n",
    "\n",
    "def generate(model, prompt,\n",
    "             max_new_tokens=95, temperature=0.7, top_p=0.9):\n",
    "    output = model.generate(sv_prompt,\n",
    "                            max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p,\n",
    "                            stop_at_eos=False if device == 'mps' else True, prepend_bos=sae.cfg.prepend_bos,\n",
    "                            return_type=\"tensor\")\n",
    "    return model.tokenizer.decode(output[0]), output\n",
    "\n",
    "def generate_with_steering(model, sae, prompt, steering_feature, max_act, steering_strength=1.0, max_new_tokens=95):\n",
    "  input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "  steering_vector = sae.W_dec[steering_feature].to(model.cfg.device)\n",
    "\n",
    "  steering_hook = partial(\n",
    "    steering, steering_vector=steering_vector, steering_strength=steering_strength, max_act=max_act\n",
    "  )\n",
    "\n",
    "  # standard transformerlens syntax for a hook context for generation\n",
    "  with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "    output = model.generate(\n",
    "      input_ids,\n",
    "      max_new_tokens=max_new_tokens,\n",
    "      temperature=0.7,\n",
    "      top_p=0.9,\n",
    "      stop_at_eos=False if device == 'mps' else True,\n",
    "      prepend_bos=sae.cfg.prepend_bos,\n",
    "    )\n",
    "\n",
    "  return model.tokenizer.decode(output[0]), output\n",
    "\n",
    "def beautify_generations(model, prompt, sae, output, color=\"blue\"):\n",
    "  input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "  # Decode the prompt and generated text separately\n",
    "  prompt_len = len(input_ids[0])\n",
    "  prompt_tokens = output[0][:prompt_len]\n",
    "  generated_tokens = output[0][prompt_len:]\n",
    "\n",
    "  prompt_text = model.tokenizer.decode(prompt_tokens)\n",
    "  generated_text = model.tokenizer.decode(generated_tokens)\n",
    "\n",
    "  # Create HTML with different colors\n",
    "  html_output = f\"\"\"\n",
    "<div style=\"font-family: monospace;\">\n",
    "    <span style=\"color: black;\">{prompt_text}</span>\n",
    "    <span style=\"color: {color}; font-weight: bold;\">{generated_text}</span>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "  # Display the colored output\n",
    "  display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323f09d-236b-45ba-9a1d-515391126c5c",
   "metadata": {},
   "source": [
    "---\n",
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb99bb-8b45-4a28-9bb8-a2b42eee1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a feature to steer\n",
    "steering_feature = steering_feature = 43499 # related to furniture\n",
    "\n",
    "# Find the maximum activation for this feature\n",
    "# NOTE: we could also get the max activation from Neuronpedia (https://www.neuronpedia.org/api-doc#tag/lookup/GET/api/feature/{modelId}/{layer}/{index})\n",
    "max_act = 50.66\n",
    "print(f'Maximum activation for feature {steering_feature}: {max_act:.4f}')\n",
    "\n",
    "# Generate text without steering for comparison\n",
    "normal_text, normal_output = generate(model, sv_prompt)\n",
    "beautify_generations(model, sv_prompt, sae, normal_output)\n",
    "\n",
    "if DEBUG: print('\\nNormal text (without steering):', normal_text)\n",
    "\n",
    "# Generate text with steering\n",
    "steered_text, steered_output = generate_with_steering(model, sae, sv_prompt, steering_feature, \n",
    "                                                      max_act, steering_strength=3.0)\n",
    "beautify_generations(model, sv_prompt, sae, steered_output, color=\"red\")\n",
    "if DEBUG: print('Steered text:\\n', steered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c773e88f-fc30-4c52-b19a-c1b88974d710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
